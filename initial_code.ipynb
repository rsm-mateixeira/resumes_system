{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by getting the documents from Google Drive using API from shared folder\n",
    "\n",
    "Source used: https://developers.google.com/drive/api/quickstart/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.oauth2.credentials import Credentials\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "    \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "FOLDER_ID = \"1YVPomt1xV_GBqE0IaUsydybqUX9WhaO7\"\n",
    "\n",
    "DOWNLOAD_DIR = \"app/static/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate function\n",
    "def authenticate():\n",
    "\n",
    "    creds = None\n",
    "\n",
    "    if os.path.exists(\"token.json\"):\n",
    "        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                \"credentials.json\", SCOPES\n",
    "            )\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open(\"token.json\", \"w\") as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    return build(\"drive\", \"v3\", credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files information\n",
    "def list_files_in_folder(service, folder_id):\n",
    "\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    return results.get(\"files\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "def download_file(service, file_id, file_name, download_path):\n",
    "\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_path = os.path.join(download_path, file_name)\n",
    "\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(request.execute())\n",
    "\n",
    "    print(f\"Downloaded: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=551693389828-kjkq02525tj6abkin3ld307bd4f72uhb.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A47313%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=337F3tmOwQd8jwGIPfLN36Kxrnrj6G&access_type=offline\n",
      "Downloaded: Resume-Samples-1-36-13.pdf\n",
      "Downloaded: Resume-Samples-1-36-25.pdf\n",
      "Downloaded: Resume-Samples-1-36-10.pdf\n",
      "Downloaded: Resume-Samples-1-36-30.pdf\n",
      "Downloaded: Resume-Samples-1-36-34.pdf\n",
      "Downloaded: Resume-Samples-1-36-17.pdf\n",
      "Downloaded: Resume-Samples-1-36-31.pdf\n",
      "Downloaded: Resume-Samples-1-36-33.pdf\n",
      "Downloaded: Resume-Samples-1-36-2.pdf\n",
      "Downloaded: Resume-Samples-1-36-26.pdf\n",
      "Downloaded: Resume-Samples-1-36-15.pdf\n",
      "Downloaded: Resume-Samples-1-36-14.pdf\n",
      "Downloaded: Resume-Samples-1-36-11.pdf\n",
      "Downloaded: Resume-Samples-1-36-35.pdf\n",
      "Downloaded: Resume-Samples-1-36-19.pdf\n",
      "Downloaded: Resume-Samples-1-36-6.pdf\n",
      "Downloaded: Resume-Samples-1-36-22.pdf\n",
      "Downloaded: Resume-Samples-1-36-4.pdf\n",
      "Downloaded: Resume-Samples-1-36-12.pdf\n",
      "Downloaded: Resume-Samples-1-36-16.pdf\n",
      "Downloaded: Resume-Samples-1-36-32.pdf\n",
      "Downloaded: Resume-Samples-1-36-3.pdf\n",
      "Downloaded: Resume-Samples-1-36-28.pdf\n",
      "Downloaded: Resume-Samples-1-36-20.pdf\n",
      "Downloaded: Resume-Samples-1-36-27.pdf\n",
      "Downloaded: Resume-Samples-1-36-7.pdf\n",
      "Downloaded: Resume-Samples-1-36-8.pdf\n",
      "Downloaded: Resume-Samples-1-36-18.pdf\n",
      "Downloaded: Resume-Samples-1-36-5.pdf\n",
      "Downloaded: Resume-Samples-1-36-9.pdf\n",
      "Downloaded: Resume-Samples-1-36-21.pdf\n",
      "Downloaded: Resume-Samples-1-36-24.pdf\n",
      "Downloaded: Resume-Samples-1-36-29.pdf\n",
      "Downloaded: Resume-Samples-1-36-23.pdf\n",
      "Downloaded: Resume-Samples-1-36-36.pdf\n",
      "\n",
      "All files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        service = authenticate()\n",
    "\n",
    "        if not os.path.exists(DOWNLOAD_DIR):\n",
    "            os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "        files = list_files_in_folder(service, FOLDER_ID)\n",
    "\n",
    "        if not files:\n",
    "            print(\"âš  No files found in the folder.\")\n",
    "            return\n",
    "\n",
    "        for file in files:\n",
    "            download_file(service, file[\"id\"], file[\"name\"], DOWNLOAD_DIR)\n",
    "      \n",
    "        print(\"\\nAll files downloaded successfully!\")\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will extract the content of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from resumes\n",
    "def extract_resume_text(folder_path):\n",
    "    resumes = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "            pages = []\n",
    "            for page in loader.load():\n",
    "                pages.append(page.page_content)  # Get text from each page\n",
    "            \n",
    "            resume_text = \" \".join(pages)  # Combine all pages into one text\n",
    "            resumes.append({\"file\": file, \"text\": resume_text})\n",
    "    return resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_resumes = extract_resume_text(DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"models/gemini-2.0-flash\", google_api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_resume(resume):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Based on the following resume, generate a structured and detailed profile suitable for matching future job descriptions. Organize the output into the following sections:\n",
    "\n",
    "        - Information: Name, email, phone number\n",
    "        - Candidate Summary: A brief professional summary highlighting key strengths, experience, and career focus.\n",
    "        - Core Skills: List of technical and soft skills relevant to job applications.\n",
    "        - Work Experience: Structured descriptions of previous roles, including job title, company name, dates of employment, responsibilities, key achievements, and technologies used.\n",
    "        - Education: Degree(s), university name, graduation year, and relevant coursework if applicable.\n",
    "        - Certifications & Training: Any relevant certifications or training programs completed.\n",
    "        - Projects & Portfolio: Notable projects, personal or professional, with a brief description of objectives and outcomes.\n",
    "        - Industry Keywords: A list of relevant industry terms, skills, and technologies that enhance job-matching capabilities.\n",
    "\n",
    "        If the user doesn't have any of the sections, go to the next section.\n",
    "        Don't add any other random text, just the answer to my request.\n",
    "\n",
    "        Here is the resume:\n",
    "\n",
    "        {resume}\n",
    "    \"\"\"\n",
    "\n",
    "    return llm.invoke(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Resume-Samples-1-36-10.pdf\n",
      "Done Resume-Samples-1-36-11.pdf\n",
      "Done Resume-Samples-1-36-12.pdf\n",
      "Done Resume-Samples-1-36-13.pdf\n",
      "Done Resume-Samples-1-36-14.pdf\n",
      "Done Resume-Samples-1-36-15.pdf\n",
      "Done Resume-Samples-1-36-16.pdf\n",
      "Done Resume-Samples-1-36-17.pdf\n",
      "Done Resume-Samples-1-36-18.pdf\n",
      "Done Resume-Samples-1-36-19.pdf\n",
      "Done Resume-Samples-1-36-2.pdf\n",
      "Done Resume-Samples-1-36-20.pdf\n",
      "Done Resume-Samples-1-36-21.pdf\n",
      "Done Resume-Samples-1-36-22.pdf\n",
      "Done Resume-Samples-1-36-23.pdf\n",
      "Done Resume-Samples-1-36-24.pdf\n",
      "Done Resume-Samples-1-36-25.pdf\n",
      "Done Resume-Samples-1-36-26.pdf\n",
      "Done Resume-Samples-1-36-27.pdf\n",
      "Done Resume-Samples-1-36-28.pdf\n",
      "Done Resume-Samples-1-36-29.pdf\n",
      "Done Resume-Samples-1-36-3.pdf\n",
      "Done Resume-Samples-1-36-30.pdf\n",
      "Done Resume-Samples-1-36-31.pdf\n",
      "Done Resume-Samples-1-36-32.pdf\n",
      "Done Resume-Samples-1-36-33.pdf\n",
      "Done Resume-Samples-1-36-34.pdf\n",
      "Done Resume-Samples-1-36-35.pdf\n",
      "Done Resume-Samples-1-36-36.pdf\n",
      "Done Resume-Samples-1-36-4.pdf\n",
      "Done Resume-Samples-1-36-5.pdf\n",
      "Done Resume-Samples-1-36-6.pdf\n",
      "Done Resume-Samples-1-36-7.pdf\n",
      "Done Resume-Samples-1-36-8.pdf\n",
      "Done Resume-Samples-1-36-9.pdf\n"
     ]
    }
   ],
   "source": [
    "for file in processed_resumes:\n",
    "    file[\"summary\"] = summarize_resume(file[\"text\"])\n",
    "    print(f\"Done {file['file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start point after getting and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to use InMemoryVectorStore because don't need data to be stored after execution, as this is a small project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"app/static/data/processed_resumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed_resumes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "docs = [Document(page_content=res[\"summary\"], metadata={\"source\": res[\"file\"]}) for res in processed_resumes]\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define state for memory and conversation memory for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    user_input: Optional[str] = Field(default=None, description=\"User's latest input message\")\n",
    "    response: Optional[List[str]] = Field(default=None, description=\"Generated response to the user\")\n",
    "    related: Optional[str] = Field(default=None, description=\"Indicates relation status of the current conversation\")\n",
    "    retrieved_files: Optional[List[Tuple[str, str]]] = Field(default=None, description=\"List of retrieved files (source, page_content)\")\n",
    "    last_action: Optional[str] = Field(default=None, description=\"Last operation performed\")\n",
    "\n",
    "    def update_user_input(self, input_text: str):\n",
    "        \"\"\"Updates the user input.\"\"\"\n",
    "        self.user_input = input_text\n",
    "\n",
    "    def update_response(self, response_text: str):\n",
    "        \"\"\"Ensures the response is stored as a list.\"\"\"\n",
    "        if isinstance(response_text, list):\n",
    "            self.response = response_text  # If already a list, keep it as is\n",
    "        else:\n",
    "            self.response = [response_text]  # Convert string to a list\n",
    "\n",
    "    def update_retrieved_files(self, files: List[Tuple[str, str]]):\n",
    "        \"\"\"Updates the list of retrieved files.\"\"\"\n",
    "        self.retrieved_files = files\n",
    "\n",
    "    def store_relation(self, relation_status: str):\n",
    "        \"\"\"Stores relation status for conversation tracking.\"\"\"\n",
    "        self.related = relation_status\n",
    "\n",
    "    def update_last_action(self, action: str):\n",
    "        \"\"\"Updates the last performed action.\"\"\"\n",
    "        self.last_action = action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_573603/2066715099.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_context(state: AgentState) -> AgentState:\n",
    "\n",
    "    memory = chat_memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Based on the message and the memory, is this request related to the one before or are we talking about a new position?\n",
    "        If the user is talking about a new position return New, otherwise Old.\n",
    "\n",
    "        User: {state.user_input}\n",
    "\n",
    "        Memory: {memory}\n",
    "\n",
    "        Return ONLY New or Old.\n",
    "    \"\"\"\n",
    "\n",
    "    related = llm.invoke(prompt).strip()\n",
    "\n",
    "    state.store_relation(related)\n",
    "\n",
    "    print(f\"check_context done: {state}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_router(state: AgentState):\n",
    "\n",
    "    print(f\"first_router done: {state}\")\n",
    "    \n",
    "    return state.related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_5_best_fit(state) -> AgentState:\n",
    "\n",
    "    # Clear old retrieved files\n",
    "    state.update_retrieved_files([])\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Structure the job description in a way that can be used to best match resumes descriptions which have the format below.\n",
    "\n",
    "        - Information: Name, email, phone number\n",
    "        - Candidate Summary: A brief professional summary highlighting key strengths, experience, and career focus.\n",
    "        - Core Skills: List of technical and soft skills relevant to job applications.\n",
    "        - Work Experience: Structured descriptions of previous roles, including job title, company name, dates of employment.\n",
    "        - Education: Degree(s), university name, graduation year.\n",
    "        - Certifications & Training: Any relevant certifications or training programs completed.\n",
    "        - Industry Keywords: A list of relevant industry terms, skills, and technologies that enhance job-matching capabilities.\n",
    "\n",
    "        Don't add any other random text, just the answer to my request.\n",
    "\n",
    "        {state.user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    text_to_match = llm.invoke(prompt).strip()\n",
    "\n",
    "    results = vector_store.similarity_search(text_to_match, k=5)\n",
    "\n",
    "    document_info = [(doc.metadata[\"source\"], doc.page_content) for doc in results]\n",
    "\n",
    "    state.update_retrieved_files(document_info)\n",
    "\n",
    "    state.update_last_action(\"find_top_5_best_fit\")\n",
    "\n",
    "    print(f\"find_top_5_best_fit last: {state}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_request(state: AgentState) -> AgentState:\n",
    "    prompt = f\"\"\"\n",
    "        Based on the request of the user, route my next step:\n",
    "\n",
    "        - If the user is giving a job description, return \"candidates_description\".\n",
    "        - If the user is asking for you to be more detailed, return \"get_details\"\n",
    "        - If he is asking specifically to answer with the resume files, return \"get_resume_by_filename\"\n",
    "\n",
    "        User Text: {state.user_input}\n",
    "\n",
    "        Return ONLY get_resume_by_filename, get_details or candidates_description.\n",
    "    \"\"\"\n",
    "\n",
    "    route = llm.invoke(prompt).strip()\n",
    "\n",
    "    print(f\"Retrieve request: {route}\")\n",
    "\n",
    "    state.update_last_action(route)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(state: AgentState) -> AgentState:\n",
    "\n",
    "    candidates_info = []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Understand from the user request, which candidates the user wants more information.\n",
    "        If the user specifies that wants from the first 3 for example, return 1,2,3.\n",
    "        If the user specify names, return the number they are in the order of the information candidates I will provide.\n",
    "\n",
    "        User Text: \n",
    "        {state.user_input}\n",
    "\n",
    "        Information candidates: \n",
    "        {state.response}\n",
    "\n",
    "        Return ONLY the numbers, with comma as separator.\n",
    "    \"\"\"\n",
    "\n",
    "    numbers = llm.invoke(prompt).strip()\n",
    "\n",
    "    numbers = numbers.replace(\" \", \"\").split(\",\")\n",
    "\n",
    "    for number in numbers:\n",
    "        candidates_info.append(state.retrieved_files[int(number) - 1][1])\n",
    "\n",
    "    state.update_response(candidates_info)\n",
    "    state.update_last_action(\"get_details\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_description(state: AgentState) -> AgentState:\n",
    "\n",
    "    page_contents = [content for _, content in state.retrieved_files]\n",
    "\n",
    "    state.update_last_action(\"candidates_description\")\n",
    "    state.update_response(page_contents) \n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_by_filename(state: AgentState) -> AgentState:\n",
    "\n",
    "    filename = [file for file, _ in state.retrieved_files]\n",
    "\n",
    "    state.update_last_action(\"get_resume_by_filename\")\n",
    "    state.update_response(filename)\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer(state: AgentState):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    if state.last_action == \"candidates_description\":\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            Based on the results of the retrieve, write a text with the structure below for each candidate retrieved.\n",
    "\n",
    "            Candidates: {state.response}\n",
    "\n",
    "            Structure to follow:\n",
    "\n",
    "            Here are the best matches based on your description:\n",
    "\n",
    "            Name candidate | phone number | email\n",
    "            Create a short and concise text why is a good fit.\n",
    "            ...\n",
    "        \"\"\"\n",
    "\n",
    "        result = llm.invoke(prompt).strip()\n",
    "\n",
    "    elif state.last_action == \"get_details\":\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            Create a detailed and structured description of the candidates.\n",
    "            Try to summarize a bit the text but keep some detail.\n",
    "\n",
    "            Candidates: {state.response}\n",
    "\n",
    "            Structure to follow:\n",
    "\n",
    "            Name candidate\n",
    "            Short summary\n",
    "            - Paragraph about work experience\n",
    "            - Paragraph about studies\n",
    "            - Paragraph about skills\n",
    "            - Another short paragraph with some information you think is relevant\n",
    "            ...\n",
    "        \"\"\"\n",
    "\n",
    "        result = llm.invoke(prompt).strip()\n",
    "\n",
    "    else:\n",
    "        files = state.response\n",
    "\n",
    "        result = f\"Here are the files: {', '.join(files)}\"\n",
    "\n",
    "    state.update_last_action(\"create_answer\")\n",
    "\n",
    "    chat_memory.save_context({\"question\": state.user_input}, {\"answer\": result})\n",
    "\n",
    "    return {'user_input': state.user_input, 'response': result, 'related': state.related, 'retrieved_files': state.retrieved_files, 'last_action': state.last_action}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_router(state: AgentState):\n",
    "\n",
    "    return state.last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__start__': Node(id='__start__', name='__start__', data=<class '__main__.AgentState'>, metadata=None), 'check_context': Node(id='check_context', name='check_context', data=check_context(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'retrieve_request': Node(id='retrieve_request', name='retrieve_request', data=retrieve_request(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'candidates_description': Node(id='candidates_description', name='candidates_description', data=candidates_description(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'get_details': Node(id='get_details', name='get_details', data=get_details(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'find_top_5_best_fit': Node(id='find_top_5_best_fit', name='find_top_5_best_fit', data=find_top_5_best_fit(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'get_resume_by_filename': Node(id='get_resume_by_filename', name='get_resume_by_filename', data=get_resume_by_filename(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'create_answer': Node(id='create_answer', name='create_answer', data=create_answer(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=<class '__main__.AgentState'>, metadata=None)}\n",
      "[Edge(source='__start__', target='check_context', data=None, conditional=False), Edge(source='candidates_description', target='create_answer', data=None, conditional=False), Edge(source='create_answer', target='__end__', data=None, conditional=False), Edge(source='find_top_5_best_fit', target='candidates_description', data=None, conditional=False), Edge(source='get_details', target='create_answer', data=None, conditional=False), Edge(source='get_resume_by_filename', target='create_answer', data=None, conditional=False), Edge(source='check_context', target='find_top_5_best_fit', data='New', conditional=True), Edge(source='check_context', target='retrieve_request', data='Old', conditional=True), Edge(source='retrieve_request', target='get_details', data=None, conditional=True), Edge(source='retrieve_request', target='get_resume_by_filename', data=None, conditional=True)]\n"
     ]
    }
   ],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"check_context\", check_context)\n",
    "graph.add_node(\"retrieve_request\", retrieve_request)\n",
    "graph.add_node(\"candidates_description\", candidates_description)\n",
    "graph.add_node(\"get_details\", get_details)\n",
    "graph.add_node(\"find_top_5_best_fit\", find_top_5_best_fit)\n",
    "graph.add_node(\"get_resume_by_filename\", get_resume_by_filename)\n",
    "graph.add_node(\"create_answer\", create_answer)\n",
    "\n",
    "\n",
    "graph.set_entry_point(\"check_context\")\n",
    "graph.add_conditional_edges(\n",
    "    \"check_context\", \n",
    "    first_router,\n",
    "    {\"New\": \"find_top_5_best_fit\", \"Old\": \"retrieve_request\"}\n",
    ")\n",
    "graph.add_edge(\"find_top_5_best_fit\", \"candidates_description\")\n",
    "graph.add_conditional_edges(\n",
    "    \"retrieve_request\", \n",
    "    second_router,\n",
    "    {\"get_details\": \"get_details\", \"get_resume_by_filename\": \"get_resume_by_filename\"}\n",
    ")\n",
    "graph.add_edge(\"candidates_description\", \"create_answer\")\n",
    "graph.add_edge(\"get_resume_by_filename\", \"create_answer\")\n",
    "graph.add_edge(\"get_details\", \"create_answer\")\n",
    "graph.set_finish_point(\"create_answer\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "\n",
    "# Debug: Print graph structure\n",
    "print(workflow.get_graph().nodes)\n",
    "print(workflow.get_graph().edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(workflow.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = AgentState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting chat...\")\n",
    "        break\n",
    "\n",
    "    state.update_user_input(user_input)\n",
    "\n",
    "    invoke_workflow = workflow.invoke(state)\n",
    "\n",
    "    state.update_retrieved_files(invoke_workflow['retrieved_files'])\n",
    "    state.update_response(invoke_workflow['response'])\n",
    "    state.update_last_action(invoke_workflow['last_action'])\n",
    "\n",
    "    print(f\"Bot: {invoke_workflow['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
